# Askl-AI-on | Ambient-Scribe: Basis-Setup

Bevor spezifische Funktionen wie die automatisierte Transkription oder die Generierung von Arztbriefen genutzt werden k√∂nnen, muss eine stabile Infrastruktur f√ºr lokale KI-Modelle geschaffen werden. Diese Anleitung f√ºhrt Sie durch die Installation der drei Grundpfeiler: **Docker**, **Ollama** und **OpenWebUI**.

> **Hinweis:** Dies ist die zwingende Voraussetzung, um KI-Modelle ohne Cloud-Anbindung direkt auf Ihrer Praxis-Hardware zu betreiben.

---

## üèóÔ∏è 1. Docker: Das Fundament

Docker ist eine Plattform, die Anwendungen in isolierten "Containern" ausf√ºhrt. Dies garantiert, dass die KI-Software unabh√§ngig von Ihrem Betriebssystem stabil l√§uft.

* **Offizielle Anleitung:** [Docker Get Started](https://docs.docker.com/get-docker/)
* **Kurzzusammenfassung:** * Laden Sie **Docker Desktop** (f√ºr Windows/Mac) oder die **Docker Engine** (f√ºr Linux) herunter.
* Installieren Sie die Anwendung und stellen Sie sicher, dass sie gestartet ist (ein kleines Wal-Symbol in der Taskleiste zeigt dies an).
* Docker verwaltet im Hintergrund die Ressourcen f√ºr OpenWebUI und n8n.



---

## üß† 2. Ollama: Das KI-Triebwerk

Ollama ist die Software, die die eigentlichen Sprachmodelle (LLMs) l√§dt und ausf√ºhrt. Sie fungiert als Schnittstelle zwischen der Hardware (Grafikkarte) und der Anwendung.

* **Offizielle Anleitung:** [Ollama Download & Setup](https://ollama.com/download)
* **Kurzzusammenfassung:**
* Laden Sie den Installer f√ºr Ihr Betriebssystem herunter und f√ºhren Sie ihn aus.
* Ollama l√§uft nach der Installation als Hintergrunddienst.
* **Test-Modell laden:** √ñffnen Sie Ihr Terminal (CMD oder PowerShell) und geben Sie folgenden Befehl ein, um ein leistungsf√§higes, kompaktes Modell f√ºr einen ersten Test zu laden:
```bash
ollama run llama3.2

```


* Sobald der Download fertig ist, k√∂nnen Sie direkt im Terminal mit der KI chatten. Geben Sie `/bye` ein, um den Chat zu beenden.



---

## üñ•Ô∏è 3. OpenWebUI: Die Benutzeroberfl√§che

OpenWebUI bietet Ihnen eine grafische Oberfl√§che f√ºr Ihre lokale KI, die optisch und funktional stark an ChatGPT erinnert ‚Äì jedoch komplett lokal in Ihrem Browser l√§uft.

* **Offizielle Anleitung:** [OpenWebUI Installation Guide](https://docs.openwebui.com/getting-started/)
* **Kurzzusammenfassung:**
* OpenWebUI wird am einfachsten √ºber Docker installiert.
* F√ºhren Sie folgenden Befehl in Ihrem Terminal aus, um OpenWebUI zu starten (dies verbindet die Oberfl√§che automatisch mit Ihrem installierten Ollama):
```bash
docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui ghcr.io/open-webui/open-webui:main

```


* Nach dem Start erreichen Sie Ihre Praxis-KI im Browser unter: `http://localhost:3000`.
* Beim ersten Aufruf m√ºssen Sie ein lokales Administratoren-Konto erstellen (Ihre Daten bleiben nur auf diesem Rechner).



---

## üìä Zusammenfassung der Infrastruktur

| Komponente | Aufgabe | Status-Check |
| --- | --- | --- |
| **Docker** | System-Umgebung | Wal-Icon in der Taskleiste sichtbar |
| **Ollama** | Modell-Hosting | Befehl `ollama list` zeigt installierte Modelle |
| **OpenWebUI** | Benutzer-Interface | Oberfl√§che unter Port 3000 im Browser erreichbar |

---

## üí° N√§chste Schritte

Sobald diese drei Komponenten laufen, ist Ihre Praxis bereit f√ºr den Einsatz von **Ambient-Scribe**.
